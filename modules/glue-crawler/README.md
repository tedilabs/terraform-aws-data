# glue-crawler

This module creates following resources.

- `aws_glue_crawler`
- `aws_iam_role` (optional)
- `aws_iam_role_policy` (optional)
- `aws_iam_role_policy_attachment` (optional)
- `aws_iam_instance_profile` (optional)

<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->
## Requirements

| Name | Version |
|------|---------|
| <a name="requirement_terraform"></a> [terraform](#requirement\_terraform) | >= 1.3 |
| <a name="requirement_aws"></a> [aws](#requirement\_aws) | >= 4.65 |

## Providers

| Name | Version |
|------|---------|
| <a name="provider_aws"></a> [aws](#provider\_aws) | 5.0.1 |

## Modules

| Name | Source | Version |
|------|--------|---------|
| <a name="module_resource_group"></a> [resource\_group](#module\_resource\_group) | tedilabs/misc/aws//modules/resource-group | ~> 0.10.0 |
| <a name="module_role"></a> [role](#module\_role) | tedilabs/account/aws//modules/iam-role | ~> 0.25.0 |

## Resources

| Name | Type |
|------|------|
| [aws_glue_crawler.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/glue_crawler) | resource |
| [aws_iam_role.custom](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_role) | data source |

## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| <a name="input_database"></a> [database](#input\_database) | (Required) The Glue database where results are written. | `string` | n/a | yes |
| <a name="input_name"></a> [name](#input\_name) | (Required) The name of the crawler. Can be up to 255 characters long. Some character set including control characters are prohibited. | `string` | n/a | yes |
| <a name="input_catalog_data_sources"></a> [catalog\_data\_sources](#input\_catalog\_data\_sources) | (Optional) A list of Glue Data Catalog data sources to be scanned by the crawler. Each item of `catalog_data_sources` as defined below.<br>    (Required) `database` - The name of the Glue Catalog database to be synchronized.<br>    (Required) `tables` - A list of one or more Glue Catalog tables to be synchronized.<br>    (Optional) `connection` - The name of the connection to use to connect to the Glue Data Catalog data source. Note that only one Catalog data source is allowed for Catalog Crawlers with a VPC connection.<br>    (Optional) `event_mode` - A configuration for event subscriptions of S3 data source which rely on Amazon S3 events to control what folders to crawl. Only required if you configured `recrawl_behavior` to `CRAWL_EVENT_MODE`.<br>      (Required) `sqs_queue` - The SQS ARN to use for identifying changes to crawl.<br>      (Optional) `sqs_dead_letter_queue` - The dead-letter SQS ARN for unprocessed messages. | <pre>list(object({<br>    database   = string<br>    tables     = set(string)<br>    connection = optional(string)<br><br>    event_mode = optional(object({<br>      sqs_queue             = string<br>      sqs_dead_letter_queue = optional(string)<br>    }))<br>  }))</pre> | `[]` | no |
| <a name="input_classifiers"></a> [classifiers](#input\_classifiers) | (Optional) A list of custom classifiers to use with this crawler. A classifier checks whether a given file is in a format the crawler can handle. If it is, the classifier creates a schema in the form of a StructType object that matches that data format. By default, all AWS classifiers are included in a crawl, but these custom classifiers always override the default classifiers for a given classification. | `set(string)` | `[]` | no |
| <a name="input_configuration"></a> [configuration](#input\_configuration) | (Optional) The configuration information of the crawler as JSON string. This versioned JSON string allows users to specify aspects of a crawler's behavior. | `string` | `""` | no |
| <a name="input_custom_iam_role"></a> [custom\_iam\_role](#input\_custom\_iam\_role) | (Optional) The IAM role friendly name (including path without leading slash), or Amazon Resource Name (ARN) of an IAM role, used by the crawler to access other resources. Provide `custom_iam_role` if you want to use the IAM role from the outside of this module. | `string` | `null` | no |
| <a name="input_data_lineage_enabled"></a> [data\_lineage\_enabled](#input\_data\_lineage\_enabled) | (Optional) Whether data lineage is enabled for the crawler. Defaults to `false`. | `bool` | `false` | no |
| <a name="input_delta_lake_data_sources"></a> [delta\_lake\_data\_sources](#input\_delta\_lake\_data\_sources) | (Optional) A list of Delta Lake data sources to be scanned by the crawler. Each item of `delta_lake_data_sources` as defined below.<br>    (Required) `paths` - A list of the Amazon S3 paths to the Delta tables.<br>    (Optional) `connection` - The name of the connection to use to connect to the Delta Lake data source.<br>    (Optional) `table_type` - How you want to create the Delta Lake tables. Valid values are `NATIVE_TABLES` and `SYMLINK_TABLES`. Defaults to `NATIVE_TABLES`.<br>      `NATIVE_TABLES` - Allow integration with query engines that support querying of the Delta transaction log directly.<br>      `SYMLINK_TABLES` - Create a symlink manifest folder with manifest files partitioned by the partition keys, based on the specified configuration parameters.<br>    (Optional) `write_manifest` - Whether to write the manifest files to the Delta table path. When enabled, if the crawler detects table metadata or schema changes in the Delta Lake transaction log, it regenerates the manifest file. You should not choose this option if you configured automatic manifest updates with Delta Lake `SET TBLPROPERTIES`. Defaults to `false`. | <pre>list(object({<br>    paths      = list(string)<br>    connection = optional(string)<br><br>    table_type     = optional(string, "NATIVE_TABLES")<br>    write_manifest = optional(bool, false)<br>  }))</pre> | `[]` | no |
| <a name="input_description"></a> [description](#input\_description) | (Optional) The description of the crawler. Can be up to 2048 characters long. | `string` | `"Managed by Terraform."` | no |
| <a name="input_dynamodb_data_sources"></a> [dynamodb\_data\_sources](#input\_dynamodb\_data\_sources) | (Optional) A list of DynamoDB data sources to be scanned by the crawler. Each item of `dynamodb_data_sources` as defined below.<br>    (Required) `path` - The name of the DynamoDB table to crawl.<br>    (Optional) `scanning_rate` - The percentage of the configured DynamoDB table Read Capacity Units to be used by the AWS Glue crawler. The valid values are between `0.1` to `1.5`. If not specified, defaults to `0.5`% for provisioned tables and 1/4 of maximum configured capacity for On-Demand tables.<br>    (Optional) `data_sampling_enabled` - Whether to scan all the records, or to sample rows from the table. Scanning all the records can take a long time when the table is not a high throughput table. Defaults to `true`. | <pre>list(object({<br>    path = string<br><br>    scanning_rate         = optional(number)<br>    data_sampling_enabled = optional(bool, true)<br>  }))</pre> | `[]` | no |
| <a name="input_iam_role"></a> [iam\_role](#input\_iam\_role) | (Optional) A configuration of the default IAM role used by the crawler to access other resources. It is only used when `custom_iam_role` is not provided. `iam_role` as defined below.<br>    (Optional) `enabled` - Whether to create a default IAM role managed by this module.<br>    (Optional) `policies` - A list of IAM policies ARNs to attach to IAM role. Defaults to `["arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole"]`.<br>    (Optional) `inline_policies` - Map of inline IAM policies to attach to IAM role. (`name` => `policy`). | <pre>object({<br>    enabled = optional(bool, true)<br>    conditions = optional(list(object({<br>      key       = string<br>      condition = string<br>      values    = list(string)<br>    })), [])<br>    policies        = optional(list(string), ["arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole"])<br>    inline_policies = optional(map(string), {})<br>  })</pre> | `{}` | no |
| <a name="input_jdbc_data_sources"></a> [jdbc\_data\_sources](#input\_jdbc\_data\_sources) | (Optional) A list of JDBC data sources to be scanned by the crawler. Each item of `jdbc_data_sources` as defined below.<br>    (Required) `path` - The path of the JDBC data source. Use `<database>/<schema>/<table>` or `<database>/<table>` format, depending on the database product. Oracle Database and MySQL donâ€™t support schema in the path. You can substitute the percent `%` character for `<schema>` or `<table>`. For example, for an Oracle database with a system identifier (SID) of orcl, enter `orcl/%` to import all tables to which the user named in the connection has access.<br>    (Optional) `connection` - The name of the connection to use to connect to the JDBC data source.<br>    (Optional) `exclusion_patterns` - A list of glob patterns used to exclude from the crawl.<br>    (Optional) `additional_metadata_properties` - A set of additional metadata properties for the crawler to crawl. Valid values are `RAWTYPES` and `COMMENTS` to enable additional metadata in table responses.<br>      `RAWTYPES` - Persist the raw datatypes of the table columns in additional metadata. As a default behavior, the crawler translates the raw datatypes to Hive-compatible types.<br>      `COMMENTS` - Crawl associated table level and column level comments. | <pre>list(object({<br>    path       = string<br>    connection = optional(string)<br><br>    exclusion_patterns             = optional(list(string), [])<br>    additional_metadata_properties = optional(set(string), [])<br>  }))</pre> | `[]` | no |
| <a name="input_lake_formation_credentials_configuration"></a> [lake\_formation\_credentials\_configuration](#input\_lake\_formation\_credentials\_configuration) | (Optional) A configuration of the crawler to use Lake Formation credentials for crawling the data source. `lake_formation_credentials_configuration` as defined below.<br>    (Optional) `enabled` - Whether to use Lake Formation credentials for the crawler instead of the IAM role credentials. Defaults to `false`.<br>    (Optional) `account_id` - A valid AWS account ID for cross account crawls. If the data source is registered in another account, you must provide the registered account ID. Otherwise, the crawler will crawl only those data sources associated to the account. | <pre>object({<br>    enabled    = optional(bool, false)<br>    account_id = optional(string, null)<br>  })</pre> | `{}` | no |
| <a name="input_module_tags_enabled"></a> [module\_tags\_enabled](#input\_module\_tags\_enabled) | (Optional) Whether to create AWS Resource Tags for the module informations. | `bool` | `true` | no |
| <a name="input_mongodb_data_sources"></a> [mongodb\_data\_sources](#input\_mongodb\_data\_sources) | (Optional) A list of MongoDB data sources to be scanned by the crawler. Each item of `mongodb_data_sources` as defined below.<br>    (Required) `path` - The path of the Amazon DocumentDB or MongoDB data source (database/collection). Use `database/collection` format.<br>    (Optional) `connection` - The name of the connection to use to connect to the Amazon DocumentDB or MongoDB data source.<br>    (Optional) `data_sampling_enabled` - Whether to scan all the records, or to sample rows from the table. Scanning all the records can take a long time when the table is not a high throughput table. Defaults to `true`. | <pre>list(object({<br>    path       = string<br>    connection = optional(string)<br><br>    data_sampling_enabled = optional(bool, true)<br>  }))</pre> | `[]` | no |
| <a name="input_on_object_deletion_behavior"></a> [on\_object\_deletion\_behavior](#input\_on\_object\_deletion\_behavior) | (Optional) A behavior type when the crawler finds a deleted object. Valid values are `LOG`, `DELETE_FROM_DATABASE` and `DEPRECATE_IN_DATABASE`. Defaults to `DEPRECATE_IN_DATABASE`.<br><br>    `LOG` - Ignore the change and don't update the table in the data catalog.<br>    `DELETE_FROM_DATABASE` - Delete tables and partitions from the data catalog.<br>    `DEPRECATE_IN_DATABASE` - Mark the table as deprecated in the data catalog. If you run a job that references a deprecated table, the job might fail. Edit jobs that reference deprecated tables to remove them as sources and targets. We recommend that you delete deprecated tables when they are no longer needed. | `string` | `"DEPRECATE_IN_DATABASE"` | no |
| <a name="input_on_recrawl_behavior"></a> [on\_recrawl\_behavior](#input\_on\_recrawl\_behavior) | (Optional) A behavior type of the crawler to recrawl from S3 data sources. Specify whether to crawl the entire dataset again, crawl only folders that were added since the last crawler run, or crawl what S3 notifies the crawler of via SQS. Valid Values are `CRAWL_EVERYTHING`, `CRAWL_EVENT_MODE` and `CRAWL_NEW_FOLDERS_ONLY`. Defaults to `CRAWL_EVERYTHING`.<br><br>    `CRAWL_EVERYTHING` - Crawl all folders again with every subsequent crawl.<br>    `CRAWL_EVENT_MODE` - Rely on Amazon S3 events to control what folders to crawl.<br>    `CRAWL_NEW_FOLDERS_ONLY` - Only Amazon S3 folders that were added since the last crawl will be crawled. If the schemas are compatible, new partitions will be added to existing tables. | `string` | `"CRAWL_EVERYTHING"` | no |
| <a name="input_on_schema_change_behavior"></a> [on\_schema\_change\_behavior](#input\_on\_schema\_change\_behavior) | (Optional) A behavior type when the crawler finds a changed schema. Valid values: `LOG` and `UPDATE_IN_DATABASE`. Defaults to `UPDATE_IN_DATABASE`.<br><br>    `LOG` - Ignore the change and don't update the table in the data catalog.<br>    `UPDATE_IN_DATABASE` - Update the table definition in the data catalog. | `string` | `"UPDATE_IN_DATABASE"` | no |
| <a name="input_resource_group_description"></a> [resource\_group\_description](#input\_resource\_group\_description) | (Optional) The description of Resource Group. | `string` | `"Managed by Terraform."` | no |
| <a name="input_resource_group_enabled"></a> [resource\_group\_enabled](#input\_resource\_group\_enabled) | (Optional) Whether to create Resource Group to find and group AWS resources which are created by this module. | `bool` | `true` | no |
| <a name="input_resource_group_name"></a> [resource\_group\_name](#input\_resource\_group\_name) | (Optional) The name of Resource Group. A Resource Group name can have a maximum of 127 characters, including letters, numbers, hyphens, dots, and underscores. The name cannot start with `AWS` or `aws`. | `string` | `""` | no |
| <a name="input_s3_data_sources"></a> [s3\_data\_sources](#input\_s3\_data\_sources) | (Optional) A list of S3 data sources to be scanned by the crawler. Each item of `s3_data_sources` as defined below.<br>    (Required) `path` - The path to the Amazon S3 data source.<br>    (Optional) `connection` - The name of a connection which allows crawler to access data in S3 within a VPC. Note that each crawler is limited to one Network connection so any other Amazon S3 targets will also use the same connection (or none, if left blank).<br>    (Optional) `exclusion_patterns` - A list of glob patterns used to exclude from the crawl.<br>    (Optional) `sample_size` - The number of files in each leaf folder to be crawled when crawling sample files in a dataset. If not set, all the files are crawled. A valid value is an integer between `1` and `249`.<br>    (Optional) `event_mode` - A configuration for event subscriptions of S3 data source which rely on Amazon S3 events to control what folders to crawl. Only required if you configured `recrawl_behavior` to `CRAWL_EVENT_MODE`.<br>      (Required) `sqs_queue` - The SQS ARN to use for identifying changes to crawl.<br>      (Optional) `sqs_dead_letter_queue` - The dead-letter SQS ARN for unprocessed messages. | <pre>list(object({<br>    path       = string<br>    connection = optional(string)<br><br>    exclusion_patterns = optional(list(string), [])<br>    sample_size        = optional(number)<br><br>    event_mode = optional(object({<br>      sqs_queue             = string<br>      sqs_dead_letter_queue = optional(string)<br>    }))<br>  }))</pre> | `[]` | no |
| <a name="input_schedule"></a> [schedule](#input\_schedule) | (Optional) A cron expression used to specify the schedule. For example, to run something every day at 12:15 UTC, you would specify: `cron(15 12 * * ? *)`. | `string` | `""` | no |
| <a name="input_security_configuration"></a> [security\_configuration](#input\_security\_configuration) | (Optional) The name of Security Configuration to be used by the crawler. Use to enable at-rest encryption on the logs pushed to CloudWatch. | `string` | `""` | no |
| <a name="input_table_prefix"></a> [table\_prefix](#input\_table\_prefix) | (Optional) The table prefix used for catalog tables that are created. | `string` | `""` | no |
| <a name="input_tags"></a> [tags](#input\_tags) | (Optional) A map of tags to add to all resources. | `map(string)` | `{}` | no |

## Outputs

| Name | Description |
|------|-------------|
| <a name="output_arn"></a> [arn](#output\_arn) | The Amazon Resource Name (ARN) of the Glue crawler. |
| <a name="output_classifiers"></a> [classifiers](#output\_classifiers) | A list of custom classifiers to use with this crawler. |
| <a name="output_configuration"></a> [configuration](#output\_configuration) | The configuration information of the crawler as JSON string. |
| <a name="output_data_lineage_enabled"></a> [data\_lineage\_enabled](#output\_data\_lineage\_enabled) | Whether data lineage is enabled for the crawler. |
| <a name="output_data_sources"></a> [data\_sources](#output\_data\_sources) | The configuration for data sources of the the crawler.<br>    `catalog` - A list of Glue Data Catalog data sources to be scanned by the crawler.<br>    `delta_lake` - A list of Delta Lake data sources to be scanned by the crawler.<br>    `dynamodb` - A list of DynamoDB data sources to be scanned by the crawler.<br>    `jdbc` - A list of JDBC data sources to be scanned by the crawler.<br>    `mongodb` - A list of MongoDB data sources to be scanned by the crawler.<br>    `s3` - A list of S3 data sources to be scanned by the crawler. |
| <a name="output_database"></a> [database](#output\_database) | The Glue database where results are written. |
| <a name="output_description"></a> [description](#output\_description) | The description of the Glue crawler. |
| <a name="output_iam_role"></a> [iam\_role](#output\_iam\_role) | The IAM role used by the crawler to access other resources. |
| <a name="output_id"></a> [id](#output\_id) | The ID of the Glue crawler. |
| <a name="output_lake_formation_credentials_configuration"></a> [lake\_formation\_credentials\_configuration](#output\_lake\_formation\_credentials\_configuration) | The configuration of the crawler to use Lake Formation credentials for crawling the data source. |
| <a name="output_name"></a> [name](#output\_name) | The name of the Glue crawler. |
| <a name="output_on_object_deletion_behavior"></a> [on\_object\_deletion\_behavior](#output\_on\_object\_deletion\_behavior) | The behavior type when the crawler finds a deleted object. |
| <a name="output_on_recrawl_behavior"></a> [on\_recrawl\_behavior](#output\_on\_recrawl\_behavior) | The behavior type of the crawler to recrawl from S3 data sources. |
| <a name="output_on_schema_change_behavior"></a> [on\_schema\_change\_behavior](#output\_on\_schema\_change\_behavior) | The behavior type when the crawler finds a changed schema. |
| <a name="output_schedule"></a> [schedule](#output\_schedule) | The cron expression used to specify the schedule. |
| <a name="output_security_configuration"></a> [security\_configuration](#output\_security\_configuration) | The name of Security Configuration of the crawler. |
| <a name="output_table_prefix"></a> [table\_prefix](#output\_table\_prefix) | The table prefix used for catalog tables that are created. |
<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->
