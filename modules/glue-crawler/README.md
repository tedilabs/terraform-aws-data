# glue-crawler

This module creates following resources.

- `aws_glue_crawler`
- `aws_iam_role` (optional)
- `aws_iam_role_policy` (optional)
- `aws_iam_role_policy_attachment` (optional)
- `aws_iam_instance_profile` (optional)

<!-- BEGIN_TF_DOCS -->
## Requirements

| Name | Version |
|------|---------|
| <a name="requirement_terraform"></a> [terraform](#requirement\_terraform) | >= 1.12 |
| <a name="requirement_aws"></a> [aws](#requirement\_aws) | >= 6.13 |

## Providers

| Name | Version |
|------|---------|
| <a name="provider_aws"></a> [aws](#provider\_aws) | 6.27.0 |

## Modules

| Name | Source | Version |
|------|--------|---------|
| <a name="module_resource_group"></a> [resource\_group](#module\_resource\_group) | tedilabs/misc/aws//modules/resource-group | ~> 0.12.0 |
| <a name="module_role"></a> [role](#module\_role) | tedilabs/account/aws//modules/iam-role | ~> 0.33.0 |

## Resources

| Name | Type |
|------|------|
| [aws_glue_crawler.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/glue_crawler) | resource |
| [aws_caller_identity.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/caller_identity) | data source |
| [aws_iam_policy_document.cloudwatch](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document) | data source |
| [aws_region.this](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/region) | data source |

## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| <a name="input_destination"></a> [destination](#input\_destination) | (Required) A configuration of the Glue database where results are written. `destination` as defined below.<br/>    (Required) `database` - The Glue database where results are written.<br/>    (Optional) `table_prefix` - The table prefix used for catalog tables that are created. This prefix will be added to table names. | <pre>object({<br/>    database     = string<br/>    table_prefix = optional(string, "")<br/>  })</pre> | n/a | yes |
| <a name="input_name"></a> [name](#input\_name) | (Required) The name of the crawler. Can be up to 255 characters long. Some character set including control characters are prohibited. | `string` | n/a | yes |
| <a name="input_catalog_data_sources"></a> [catalog\_data\_sources](#input\_catalog\_data\_sources) | (Optional) A list of Glue Data Catalog data sources to be scanned by the crawler. Each item of `catalog_data_sources` as defined below.<br/>    (Required) `database` - The name of the Glue Catalog database to be synchronized.<br/>    (Required) `tables` - A list of one or more Glue Catalog tables to be synchronized.<br/>    (Optional) `connection` - The name of the connection to use to connect to the Glue Data Catalog data source. Note that only one Catalog data source is allowed for Catalog Crawlers with a VPC connection.<br/>    (Optional) `event_mode` - A configuration for event subscriptions of S3 data source which rely on Amazon S3 events to control what folders to crawl. Only required if you configured `recrawl_behavior` to `CRAWL_EVENT_MODE`.<br/>      (Required) `sqs_queue` - The SQS ARN to use for identifying changes to crawl.<br/>      (Optional) `sqs_dead_letter_queue` - The dead-letter SQS ARN for unprocessed messages. | <pre>list(object({<br/>    database   = string<br/>    tables     = set(string)<br/>    connection = optional(string)<br/><br/>    event_mode = optional(object({<br/>      sqs_queue             = string<br/>      sqs_dead_letter_queue = optional(string)<br/>    }))<br/>  }))</pre> | `[]` | no |
| <a name="input_configuration"></a> [configuration](#input\_configuration) | (Optional) The configuration information of the crawler as JSON string. This versioned JSON string allows users to specify aspects of a crawler's behavior. | `string` | `""` | no |
| <a name="input_custom_classifiers"></a> [custom\_classifiers](#input\_custom\_classifiers) | (Optional) A set of custom classifiers to use with this crawler. A classifier checks whether a given file is in a format the crawler can handle. If it is, the classifier creates a schema in the form of a StructType object that matches that data format. By default, all AWS classifiers are included in a crawl, but these custom classifiers always override the default classifiers for a given classification. | `set(string)` | `[]` | no |
| <a name="input_data_lineage_enabled"></a> [data\_lineage\_enabled](#input\_data\_lineage\_enabled) | (Optional) Whether data lineage is enabled for the crawler. Defaults to `false`. | `bool` | `false` | no |
| <a name="input_default_service_role"></a> [default\_service\_role](#input\_default\_service\_role) | (Optional) A configuration for the default service role used by the crawler to access other resources. Use `service_role` if `default_service_role.enabled` is `false`. `default_service_role` as defined below.<br/>    (Optional) `enabled` - Whether to create the default service role. Defaults to `true`.<br/>    (Optional) `name` - The name of the default service role. Defaults to `glue-crawler-${var.name}`.<br/>    (Optional) `path` - The path of the default service role. Defaults to `/`.<br/>    (Optional) `description` - The description of the default service role.<br/>    (Optional) `policies` - A list of IAM policy ARNs to attach to the default service role. `AWSGlueServiceRole` is always attached. Defaults to `[]`.<br/>    (Optional) `inline_policies` - A Map of inline IAM policies to attach to the default service role. (`name` => `policy`). | <pre>object({<br/>    enabled     = optional(bool, true)<br/>    name        = optional(string)<br/>    path        = optional(string, "/")<br/>    description = optional(string, "Managed by Terraform.")<br/><br/>    policies        = optional(list(string), [])<br/>    inline_policies = optional(map(string), {})<br/>  })</pre> | `{}` | no |
| <a name="input_delta_lake_data_sources"></a> [delta\_lake\_data\_sources](#input\_delta\_lake\_data\_sources) | (Optional) A list of Delta Lake data sources to be scanned by the crawler. Each item of `delta_lake_data_sources` as defined below.<br/>    (Required) `paths` - A list of the Amazon S3 paths to the Delta tables.<br/>    (Optional) `connection` - The name of the connection to use to connect to the Delta Lake data source.<br/>    (Optional) `table_type` - How you want to create the Delta Lake tables. Valid values are `NATIVE_TABLES` and `SYMLINK_TABLES`. Defaults to `NATIVE_TABLES`.<br/>      `NATIVE_TABLES` - Allow integration with query engines that support querying of the Delta transaction log directly.<br/>      `SYMLINK_TABLES` - Create a symlink manifest folder with manifest files partitioned by the partition keys, based on the specified configuration parameters.<br/>    (Optional) `write_manifest` - Whether to write the manifest files to the Delta table path. When enabled, if the crawler detects table metadata or schema changes in the Delta Lake transaction log, it regenerates the manifest file. You should not choose this option if you configured automatic manifest updates with Delta Lake `SET TBLPROPERTIES`. Defaults to `false`. | <pre>list(object({<br/>    paths      = list(string)<br/>    connection = optional(string)<br/><br/>    table_type     = optional(string, "NATIVE_TABLES")<br/>    write_manifest = optional(bool, false)<br/>  }))</pre> | `[]` | no |
| <a name="input_description"></a> [description](#input\_description) | (Optional) The description of the crawler. Can be up to 2048 characters long. | `string` | `"Managed by Terraform."` | no |
| <a name="input_dynamodb_data_sources"></a> [dynamodb\_data\_sources](#input\_dynamodb\_data\_sources) | (Optional) A list of DynamoDB data sources to be scanned by the crawler. Each item of `dynamodb_data_sources` as defined below.<br/>    (Required) `path` - The name of the DynamoDB table to crawl.<br/>    (Optional) `scanning_rate` - The percentage of the configured DynamoDB table Read Capacity Units to be used by the AWS Glue crawler. The valid values are between `0.1` to `1.5`. If not specified, defaults to `0.5`% for provisioned tables and 1/4 of maximum configured capacity for On-Demand tables.<br/>    (Optional) `data_sampling_enabled` - Whether to scan all the records, or to sample rows from the table. Scanning all the records can take a long time when the table is not a high throughput table. Defaults to `true`. | <pre>list(object({<br/>    path = string<br/><br/>    scanning_rate         = optional(number)<br/>    data_sampling_enabled = optional(bool, true)<br/>  }))</pre> | `[]` | no |
| <a name="input_jdbc_data_sources"></a> [jdbc\_data\_sources](#input\_jdbc\_data\_sources) | (Optional) A list of JDBC data sources to be scanned by the crawler. Each item of `jdbc_data_sources` as defined below.<br/>    (Required) `path` - The path of the JDBC data source. Use `<database>/<schema>/<table>` or `<database>/<table>` format, depending on the database product. Oracle Database and MySQL donâ€™t support schema in the path. You can substitute the percent `%` character for `<schema>` or `<table>`. For example, for an Oracle database with a system identifier (SID) of orcl, enter `orcl/%` to import all tables to which the user named in the connection has access.<br/>    (Optional) `connection` - The name of the connection to use to connect to the JDBC data source.<br/>    (Optional) `exclusion_patterns` - A list of glob patterns used to exclude from the crawl.<br/>    (Optional) `additional_metadata_properties` - A set of additional metadata properties for the crawler to crawl. Valid values are `RAWTYPES` and `COMMENTS` to enable additional metadata in table responses.<br/>      `RAWTYPES` - Persist the raw datatypes of the table columns in additional metadata. As a default behavior, the crawler translates the raw datatypes to Hive-compatible types.<br/>      `COMMENTS` - Crawl associated table level and column level comments. | <pre>list(object({<br/>    path       = string<br/>    connection = optional(string)<br/><br/>    exclusion_patterns             = optional(list(string), [])<br/>    additional_metadata_properties = optional(set(string), [])<br/>  }))</pre> | `[]` | no |
| <a name="input_lake_formation_credentials"></a> [lake\_formation\_credentials](#input\_lake\_formation\_credentials) | (Optional) A configuration of the crawler to use Lake Formation credentials for crawling the data source. `lake_formation_credentials` as defined below.<br/>    (Optional) `enabled` - Whether to use Lake Formation credentials for the crawler instead of the IAM role credentials. Defaults to `false`.<br/>    (Optional) `account_id` - A valid AWS account ID for cross account crawls. If the data source is registered in another account, you must provide the registered account ID. Otherwise, the crawler will crawl only those data sources associated to the account. | <pre>object({<br/>    enabled    = optional(bool, false)<br/>    account_id = optional(string, null)<br/>  })</pre> | `{}` | no |
| <a name="input_module_tags_enabled"></a> [module\_tags\_enabled](#input\_module\_tags\_enabled) | (Optional) Whether to create AWS Resource Tags for the module informations. | `bool` | `true` | no |
| <a name="input_mongodb_data_sources"></a> [mongodb\_data\_sources](#input\_mongodb\_data\_sources) | (Optional) A list of MongoDB data sources to be scanned by the crawler. Each item of `mongodb_data_sources` as defined below.<br/>    (Required) `path` - The path of the Amazon DocumentDB or MongoDB data source (database/collection). Use `database/collection` format.<br/>    (Optional) `connection` - The name of the connection to use to connect to the Amazon DocumentDB or MongoDB data source.<br/>    (Optional) `data_sampling_enabled` - Whether to scan all the records, or to sample rows from the table. Scanning all the records can take a long time when the table is not a high throughput table. Defaults to `true`. | <pre>list(object({<br/>    path       = string<br/>    connection = optional(string)<br/><br/>    data_sampling_enabled = optional(bool, true)<br/>  }))</pre> | `[]` | no |
| <a name="input_recrawl_behavior"></a> [recrawl\_behavior](#input\_recrawl\_behavior) | (Optional) A behavior type of the crawler whether to crawl the entire dataset again, or to crawl only folders that were added since the last crawler run, or crawl what S3 notifies the crawler of via SQS. Valid Values are `CRAWL_EVERYTHING`, `CRAWL_EVENT_MODE` and `CRAWL_NEW_FOLDERS_ONLY`. Defaults to `CRAWL_EVERYTHING`.<br/><br/>    `CRAWL_EVERYTHING` - Crawl all folders again with every subsequent crawl.<br/>    `CRAWL_EVENT_MODE` - Rely on Amazon S3 events to control what folders to crawl.<br/>    `CRAWL_NEW_FOLDERS_ONLY` - Only Amazon S3 folders that were added since the last crawl will be crawled. If the schemas are compatible, new partitions will be added to existing tables. | `string` | `"CRAWL_EVERYTHING"` | no |
| <a name="input_region"></a> [region](#input\_region) | (Optional) The region in which to create the module resources. If not provided, the module resources will be created in the provider's configured region. | `string` | `null` | no |
| <a name="input_resource_group"></a> [resource\_group](#input\_resource\_group) | (Optional) A configurations of Resource Group for this module. `resource_group` as defined below.<br/>    (Optional) `enabled` - Whether to create Resource Group to find and group AWS resources which are created by this module. Defaults to `true`.<br/>    (Optional) `name` - The name of Resource Group. A Resource Group name can have a maximum of 127 characters, including letters, numbers, hyphens, dots, and underscores. The name cannot start with `AWS` or `aws`. If not provided, a name will be generated using the module name and instance name.<br/>    (Optional) `description` - The description of Resource Group. Defaults to `Managed by Terraform.`. | <pre>object({<br/>    enabled     = optional(bool, true)<br/>    name        = optional(string, "")<br/>    description = optional(string, "Managed by Terraform.")<br/>  })</pre> | `{}` | no |
| <a name="input_s3_data_sources"></a> [s3\_data\_sources](#input\_s3\_data\_sources) | (Optional) A list of S3 data sources to be scanned by the crawler. Each item of `s3_data_sources` as defined below.<br/>    (Required) `path` - The path to the Amazon S3 data source.<br/>    (Optional) `connection` - The name of a connection which allows crawler to access data in S3 within a VPC. Note that each crawler is limited to one Network connection so any other Amazon S3 targets will also use the same connection (or none, if left blank).<br/>    (Optional) `exclusion_patterns` - A list of glob patterns used to exclude from the crawl.<br/>    (Optional) `sample_size` - The number of files in each leaf folder to be crawled when crawling sample files in a dataset. If not set, all the files are crawled. A valid value is an integer between `1` and `249`.<br/>    (Optional) `event_mode` - A configuration for event subscriptions of S3 data source which rely on Amazon S3 events to control what folders to crawl. Only required if you configured `recrawl_behavior` to `CRAWL_EVENT_MODE`.<br/>      (Required) `sqs_queue` - The SQS ARN to use for identifying changes to crawl.<br/>      (Optional) `sqs_dead_letter_queue` - The dead-letter SQS ARN for unprocessed messages. | <pre>list(object({<br/>    path       = string<br/>    connection = optional(string)<br/><br/>    exclusion_patterns = optional(list(string), [])<br/>    sample_size        = optional(number)<br/><br/>    event_mode = optional(object({<br/>      sqs_queue             = string<br/>      sqs_dead_letter_queue = optional(string)<br/>    }))<br/>  }))</pre> | `[]` | no |
| <a name="input_schedule"></a> [schedule](#input\_schedule) | (Optional) A configuration to schedule the crawler. `schedule` as defined below.<br/>    (Optional) `type` - The schedule type. Valid values are `ON_DEMAND` and `CRON_EXPRESSION`. Defaults to `ON_DEMAND`.<br/>      `ON_DEMAND` - The crawler runs only when explicitly started.<br/>      `CRON_EXPRESSION` - The crawler runs at the time(s) specified in the `expression`.<br/>    (Optional) `expression` - A cron expression used to specify the schedule. For example, to run something every day at 12:15 UTC, you would specify: `cron(15 12 * * ? *)`. | <pre>object({<br/>    type       = optional(string, "ON_DEMAND")<br/>    expression = optional(string, "")<br/>  })</pre> | `{}` | no |
| <a name="input_schema_change_policy"></a> [schema\_change\_policy](#input\_schema\_change\_policy) | (Optional) A configuration of the crawler's schema change policy. `schema_change_policy` as defined below.<br/>    (Optional) `delete_behavior` - The deletion behavior when the crawler finds a deleted object. Valid values are `LOG`, `DELETE_FROM_DATABASE` and `DEPRECATE_IN_DATABASE`. Defaults to `DEPRECATE_IN_DATABASE`.<br/><br/>      `LOG` - Ignore the change and don't update the table in the data catalog.<br/>      `DELETE_FROM_DATABASE` - Delete tables and partitions from the data catalog.<br/>      `DEPRECATE_IN_DATABASE` - Mark the table as deprecated in the data catalog. If you run a job that references a deprecated table, the job might fail. Edit jobs that reference deprecated tables to remove them as sources and targets. We recommend that you delete deprecated tables when they are no longer needed.<br/>    (Optional) `update_behavior` - The update behavior when the crawler finds a changed schema. Valid values: `LOG` and `UPDATE_IN_DATABASE`. Defaults to `UPDATE_IN_DATABASE`.<br/><br/>      `LOG` - Ignore the change and don't update the table in the data catalog.<br/>      `UPDATE_IN_DATABASE` - Update the table definition in the data catalog. | <pre>object({<br/>    delete_behavior = optional(string, "DEPRECATE_IN_DATABASE")<br/>    update_behavior = optional(string, "UPDATE_IN_DATABASE")<br/>  })</pre> | `{}` | no |
| <a name="input_security_configuration"></a> [security\_configuration](#input\_security\_configuration) | (Optional) The name of Security Configuration to be used by the crawler. Use to enable at-rest encryption on the logs pushed to CloudWatch. | `string` | `""` | no |
| <a name="input_service_role"></a> [service\_role](#input\_service\_role) | (Optional) The ARN (Amazon Resource Name) of the IAM Role used by the crawler to access other resources. Only required if `default_service_role.enabled` is `false`. | `string` | `null` | no |
| <a name="input_tags"></a> [tags](#input\_tags) | (Optional) A map of tags to add to all resources. | `map(string)` | `{}` | no |

## Outputs

| Name | Description |
|------|-------------|
| <a name="output_arn"></a> [arn](#output\_arn) | The Amazon Resource Name (ARN) of the Glue crawler. |
| <a name="output_configuration"></a> [configuration](#output\_configuration) | The configuration information of the crawler as JSON string. |
| <a name="output_custom_classifiers"></a> [custom\_classifiers](#output\_custom\_classifiers) | A list of custom classifiers to use with this crawler. |
| <a name="output_data_lineage_enabled"></a> [data\_lineage\_enabled](#output\_data\_lineage\_enabled) | Whether data lineage is enabled for the crawler. |
| <a name="output_data_sources"></a> [data\_sources](#output\_data\_sources) | The configuration for data sources of the the crawler.<br/>    `catalog` - A list of Glue Data Catalog data sources to be scanned by the crawler.<br/>    `delta_lake` - A list of Delta Lake data sources to be scanned by the crawler.<br/>    `dynamodb` - A list of DynamoDB data sources to be scanned by the crawler.<br/>    `jdbc` - A list of JDBC data sources to be scanned by the crawler.<br/>    `mongodb` - A list of MongoDB data sources to be scanned by the crawler.<br/>    `s3` - A list of S3 data sources to be scanned by the crawler. |
| <a name="output_description"></a> [description](#output\_description) | The description of the Glue crawler. |
| <a name="output_destination"></a> [destination](#output\_destination) | The destination information of the crawler. |
| <a name="output_id"></a> [id](#output\_id) | The ID of the Glue crawler. |
| <a name="output_lake_formation_credentials"></a> [lake\_formation\_credentials](#output\_lake\_formation\_credentials) | The configuration of the crawler to use Lake Formation credentials for crawling the data source. |
| <a name="output_name"></a> [name](#output\_name) | The name of the Glue crawler. |
| <a name="output_recrawl_behavior"></a> [recrawl\_behavior](#output\_recrawl\_behavior) | The behavior type of the crawler to recrawl from S3 data sources. |
| <a name="output_region"></a> [region](#output\_region) | The AWS region this module resources resides in. |
| <a name="output_resource_group"></a> [resource\_group](#output\_resource\_group) | The resource group created to manage resources in this module. |
| <a name="output_schedule"></a> [schedule](#output\_schedule) | The cron expression used to specify the schedule. |
| <a name="output_schema_change_policy"></a> [schema\_change\_policy](#output\_schema\_change\_policy) | The configuration of the crawler's behavior when it detects a change in a table schema. |
| <a name="output_security_configuration"></a> [security\_configuration](#output\_security\_configuration) | The name of Security Configuration of the crawler. |
| <a name="output_service_role"></a> [service\_role](#output\_service\_role) | The Amazon Resource Name (ARN) of the IAM role used by the crawler to access other esources. |
<!-- END_TF_DOCS -->
